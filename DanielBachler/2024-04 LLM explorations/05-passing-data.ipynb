{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6e75dcf-67a6-49f1-b352-73778373c10a",
   "metadata": {},
   "source": [
    "# Using data in the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84928e8b-7689-420c-802a-54f625981332",
   "metadata": {},
   "source": [
    "## Ways of using ChatGPT with data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bda359-a3f3-4bce-886d-e56f08fbd582",
   "metadata": {},
   "source": [
    "If we have data, there are multiple ways of working with ChatGPT. We will probably have already have talked about two:\n",
    "\n",
    "- Asking ChatGPT to analyse a data file\n",
    "- Asking ChatGPT to summarise an image of plotted data\n",
    "\n",
    "Both of these are human-like ways of understanding and working with data. However, ChatGPT is also able to work with data by looking at the entire raw dataset as a gestalt. This is what is happening when you pass in raw data into the prompt. That's what we'll experiment with in this session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8679f5-68ed-40ee-b827-9908c2a389f6",
   "metadata": {},
   "source": [
    "## Fighting the context window\n",
    "\n",
    "- When we pass data into the prompt, the context window becomes a problem\n",
    "- Today we will use `gpt-4-turbo` which has 128k context window\n",
    "- You will find that not enough for many things you want to do\n",
    "- Much of this session will be about deciding\n",
    "  - What do I want to show ChatGPT?\n",
    "  - In what format should I pass it?\n",
    "  - Is there any way of reducing the data size so that I can show more?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d82fc7-19f0-4389-9503-c3a772503ebc",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4601daba-a9ea-46ee-b307-4c963f122a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional\n",
    "import requests\n",
    "import sh\n",
    "from ipywidgets import interact, Dropdown\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62330b7-927d-46a5-be3c-33183889926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd903bdb-4251-445b-ab69-ae368128963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d1051d-c831-44f2-a116-b2ce81d6ed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shelve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eda8ef-38be-4510-b7b8-917a3fdd57c3",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d93a239-d24f-403f-98d6-53b70cb7779e",
   "metadata": {},
   "source": [
    "You can mostly ignore this code to start with. What it does is to provide the helper method `fetch_bundle()` that will fetch all the config, data and metadata for a single Grapher chart (or indicator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aed7fd-4cef-45c6-875f-db85dbb13c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Indicator:\n",
    "    data: dict\n",
    "    metadata: dict\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\"data\": self.data, \"metadata\": self.metadata}\n",
    "\n",
    "    def to_frame(self):\n",
    "        # getting a data frame is easy\n",
    "        df = pd.DataFrame.from_dict(self.data)\n",
    "\n",
    "        # turning entity ids into entity names\n",
    "        entities = pd.DataFrame.from_records(self.metadata['dimensions']['entities']['values'])\n",
    "        id_to_name = entities.set_index('id').name.to_dict()\n",
    "        df['entities'] = df.entities.apply(id_to_name.__getitem__)\n",
    "\n",
    "        # make the \"values\" column more interestingly named\n",
    "        df = df.rename(columns={'values': self.metadata.get('shortName', f'ind_{self.metadata[\"id\"]}')})\n",
    "\n",
    "        # order the columns better\n",
    "        cols = ['entities', 'years'] + sorted([c for c in df.columns if c not in ['entities', 'years']])\n",
    "        df = df[cols]\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GrapherBundle:\n",
    "    config: Optional[dict]\n",
    "    dimensions: Dict[int, Indicator]\n",
    "    origins: List[dict]\n",
    "\n",
    "    def to_json(self):\n",
    "        return json.dumps(\n",
    "            {\n",
    "                \"config\": self.config,\n",
    "                \"dimensions\": {k: i.to_dict() for k, i in self.dimensions.items()},\n",
    "                \"origins\": self.origins,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.to_json())\n",
    "\n",
    "    @property\n",
    "    def indicators(self) -> List[Indicator]:\n",
    "        return list(self.dimensions.values())\n",
    "\n",
    "    def to_frame(self):\n",
    "        df = None\n",
    "        for i in self.indicators:\n",
    "            to_merge = i.to_frame()\n",
    "            if df is None:\n",
    "                df = to_merge\n",
    "            else:\n",
    "                df = pd.merge(df, to_merge, how='outer', on=['entities', 'years'])\n",
    "        return df\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'GrapherBundle(config={self.config}, dimensions=..., origins=...)'\n",
    "\n",
    "def fetch_grapher_config(slug):\n",
    "    resp = requests.get(f\"https://ourworldindata.org/grapher/{slug}\")\n",
    "    resp.raise_for_status()\n",
    "    return json.loads(resp.content.decode(\"utf-8\").split(\"//EMBEDDED_JSON\")[1])\n",
    "\n",
    "\n",
    "def fetch_dimension(id: int) -> Indicator:\n",
    "    data = requests.get(\n",
    "        f\"https://api.ourworldindata.org/v1/indicators/{id}.data.json\"\n",
    "    ).json()\n",
    "    metadata = requests.get(\n",
    "        f\"https://api.ourworldindata.org/v1/indicators/{id}.metadata.json\"\n",
    "    ).json()\n",
    "    return Indicator(data, metadata)\n",
    "\n",
    "\n",
    "def fetch_bundle(\n",
    "    slug: Optional[str] = None, indicator_id: Optional[int] = None\n",
    ") -> GrapherBundle:\n",
    "    if slug:\n",
    "        config = fetch_grapher_config(slug)\n",
    "        indicator_ids = [d[\"variableId\"] for d in config[\"dimensions\"]]\n",
    "    else:\n",
    "        print(f\"Fetching indicator {indicator_id}\")\n",
    "        config = None\n",
    "        indicator_ids = [indicator_id]\n",
    "    dimensions = {\n",
    "        indicator_id: fetch_dimension(indicator_id) for indicator_id in indicator_ids\n",
    "    }\n",
    "    origins = []\n",
    "    for d in dimensions.values():\n",
    "        if d.metadata.get(\"origins\"):\n",
    "            origins.append(d.metadata.pop(\"origins\"))\n",
    "    return GrapherBundle(config, dimensions, origins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431fff0d-3116-4800-a8b9-8eebce3ffa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(slug=None, indicator_id=None):\n",
    "    key = f'{slug}::{indicator_id}'\n",
    "    with shelve.open('cache.db') as shelf:\n",
    "        if key not in shelf:\n",
    "            b = fetch_bundle(slug=slug, indicator_id=indicator_id)\n",
    "            shelf[key] = b\n",
    "        \n",
    "        return shelf[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd42b2be-bdf8-4f52-819c-5d88bcc8971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_clipboard(s):\n",
    "    sh.pbcopy(_in=s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca36ad5-5813-4ac4-ad3f-076c325b966c",
   "metadata": {},
   "source": [
    "## Helpers: asking ChatGPT by API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c372122-f1e1-48b8-89e9-069900bb7721",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7da591b-cfc1-4291-9283-a3cb0d931b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'gpt-4-turbo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1063af7-56c7-4539-961e-1892b43e6e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_response(message: str, model: str = MODEL) -> str:\n",
    "    return client.chat.completions.create(\n",
    "      model=model,\n",
    "      messages=[{\"role\": \"user\", \"content\": message}],\n",
    "    ).choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56add898-e878-4e5b-bc5a-1c61d363a8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gpt_response('Tell me a funny story in a single haiku with a surprising twist'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c5e1f0-5b24-403b-9878-46de929d7ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_cached(message: str, model: str = MODEL) -> str:\n",
    "    with shelve.open('cache.db') as shelf:\n",
    "        key = hashlib.md5(f'{model}:::{message}'.encode('utf8')).hexdigest()\n",
    "        if key in shelf:\n",
    "            return shelf[key]\n",
    "\n",
    "        resp = gpt_response(message, model)\n",
    "        shelf[key] = resp\n",
    "        return resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffc9b8b-0267-40ea-a1e8-bd811166ce39",
   "metadata": {},
   "source": [
    "## Test data fetching\n",
    "\n",
    "Let's check to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b63bb2-dbd9-4111-a2c1-bcdce0e5302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch just one indicator\n",
    "b = fetch(slug='gdp-per-capita-maddison')\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12632e48-9bb4-4aef-9a73-638c73ae6ec8",
   "metadata": {},
   "source": [
    "#### If we passed everything as JSON to ChatGPT, how big would it be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a556d5-cbe7-4bc1-b0e4-70f24c78e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(b.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c479eb-d25d-4003-b2d5-87cf79cebe65",
   "metadata": {},
   "source": [
    "395k! Much bigger than our 128k context window!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5214e52-abe0-4282-ad63-a98246f80b55",
   "metadata": {},
   "source": [
    "#### What about just the data, no metadata?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34388530-6682-46e1-b48d-4a317841119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ccc8b-663c-4c7b-bd1a-a07bfa0fa72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(b.to_frame().to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baded7f5-480e-4c17-8ef3-d60e4c9d6060",
   "metadata": {},
   "source": [
    "919k??? Eeek, it's worse, since entity names are strings now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5684691-496f-42d3-bd2e-c07a33c0a055",
   "metadata": {},
   "source": [
    "#### What about stacked area charts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dda656-f81e-4226-8c99-e888ed37964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch a stacked chart that uses a bunch of indicators\n",
    "b = fetch(slug='births-by-age-of-mother')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfedd20-39e1-400b-b4df-dd5a41e7cedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(b.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24757153-5c78-48ee-aae6-f8b84b880b0a",
   "metadata": {},
   "source": [
    "2.8MB!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4334c8a0-4360-44b2-b493-402fe2bef821",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(b.to_frame().to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac06ae9-e2ad-4f33-887a-ff7bce905164",
   "metadata": {},
   "source": [
    "2.95MB!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ba2a5f-7d0f-4c03-802a-a6d572cf6539",
   "metadata": {},
   "source": [
    "So, we will need to think carefully about what we might use in a prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46d10f4-d9c7-47d5-9e77-f96cd34529b0",
   "metadata": {},
   "source": [
    "## Part 1: making data to prompt on\n",
    "\n",
    "See what makes data bigger or smaller, when being passed to ChatGPT. Can you find a strategy that reduces this entire data file to something we can use in a prompt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e3eb3e-06e2-4771-b0fa-f9db41a25d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "slug_whitelist = set(json.load(open('slugs.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79126b5c-4a14-400a-92be-e730eb1fd44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_slug = 'life-expectancy'\n",
    "last_prompt = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64300ad0-0d16-4c32-9194-72fa0515f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(slug=last_slug)\n",
    "def find_data(slug=None):\n",
    "    global last_slug, last_prompt\n",
    "    \n",
    "    if not slug:\n",
    "        return\n",
    "\n",
    "    last_slug = slug\n",
    "    \n",
    "    if slug not in slug_whitelist:\n",
    "        matches = sorted([s for s in slug_whitelist if s.startswith(slug)])[:5]\n",
    "        if matches:\n",
    "            print('\\n'.join(matches))\n",
    "        else:\n",
    "            print('(not found)')\n",
    "        return\n",
    "    \n",
    "    b = fetch(slug=slug)\n",
    "    df = b.to_frame()\n",
    "\n",
    "    ### ------ YOU WORK HERE -------\n",
    "\n",
    "    last_prompt = df.to_json()\n",
    "    \n",
    "    ###\n",
    "\n",
    "    # let's see how we did\n",
    "    l = len(data_to_prompt(df)) // 1000\n",
    "    emoji = '❌' if l > 128 else '✅'\n",
    "    print(f'Length: {l}k {emoji}\\n')\n",
    "    print(last_prompt[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9cadfb-e2cc-43b1-86ea-faeafe61baf4",
   "metadata": {},
   "source": [
    "If you get a tick, you may copy your prompt to the clipboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204bf9c7-c7b3-4080-b414-8ce179653096",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_clipboard(last_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9334338d-d079-4c0d-8084-5a2371f70536",
   "metadata": {},
   "source": [
    "Then go paste it into ChatGPT, with any instructions you like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895c88d5-0d66-44b9-868e-ae83bbdcb202",
   "metadata": {},
   "source": [
    "#### Strategies\n",
    "\n",
    "- You could try varying the serialisation format\n",
    "  - Look at what you're really passing to ChatGPT, is that the most compact thing?\n",
    "- You could try showing only part of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceca38e-2b40-4fa9-a99f-ded90c81d222",
   "metadata": {},
   "source": [
    "## Part 2: experimenting with prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7592ccc4-3f4e-46ae-8af9-caafd7c2fa3f",
   "metadata": {},
   "source": [
    "Although we can use the ChatGPT API, you may enjoy continuing to use your clipboard and using ChatGPT more interactively.\n",
    "\n",
    "Now we want to work out what we can say to ChatGPT that will give us the kind of output we want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a070fc6a-a9ca-43e1-88ba-7e5ad1d39c2b",
   "metadata": {},
   "source": [
    "## Things to try"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d1b836-6524-4757-9b4f-a3f02898a815",
   "metadata": {},
   "source": [
    "What types of prompts work well?\n",
    "- Try giving a lot of guidance\n",
    "- Try giving little to no guidance\n",
    "- Try comparing a country to its peers, income group or neighbours (see: `peers.json`)\n",
    "- Try asking it to think step by step, then give an answer after '---'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
